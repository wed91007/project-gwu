\documentclass{article}

\usepackage{graphicx} % Required for the inclusion of images
%\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage{float}
\usepackage{url}
\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Project 2 Report} % Title

\author{Xiangyang \textsc{Han}} % Author name

\date{\today} % Date for the report

\begin{document}

\maketitle % Insert the title, author and date


% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}

%----------------------------------------------------------------------------------------
%	SECTION 1 Overview
%----------------------------------------------------------------------------------------
\section{Project Overview}


\begin{tabular}{ll}
Name:&Xiangyang Han\\
Student ID:&G29562597\\
Major Resource:&ide:Spyder; Reference:wiki,stackoverflow\\
Programming Language:&python 2.7\\
Github url:&\url{https://github.com/wed91007/project-gwu}
\end{tabular}


% If you have more than one objective, uncomment the below:
%\begin{description}
%\item[First Objective] \hfill \\
%Objective 1 text
%\item[Second Objective] \hfill \\
%Objective 2 text
%\end{description}


 
%----------------------------------------------------------------------------------------
%	SECTION 2 Question 1
%----------------------------------------------------------------------------------------

\section{Parameter Estimation}

\subsection{Problem 1}
Poisson distribution has pmf as: \\
\begin{displaymath}
P(X|\lambda)=\frac{{\lambda^X}{e^{-\lambda}}}{X!}
\end{displaymath}
MLE for Poisson distribution is getting $\lambda$ for $\max P(X|\lambda)$ with knowing X.\\
Since the logarithm function is monotonically increasing, we can use log to compute MLE:\\
\begin{align*}
L(\lambda)&=\log{\prod_{i=1}^n}f(X_i|\lambda)\\
			&=\sum_{i=1}^n \log(\frac{\lambda^X e^{-\lambda}}{X!})\\
			&=-n\lambda + (\sum_{i=1}^n X_i)\log(\lambda)-\log(\sum_{i=1}^n X!)
\end{align*}\\

When derivative of $L(\lambda)=0$, it is the max:\\
\begin{displaymath}	
\frac{d}{d\lambda}L(\lambda)=0 \Leftrightarrow -n+(\sum_{i=1}^n X_i)\frac{1}{\lambda}
\end{displaymath}
Solving the equation:\\
\begin{displaymath}
\lambda_{MLE}=\hat{\lambda}=\frac{1}{n}\sum_{i=1}^n X_i
\end{displaymath}\\

To show the result is unbiased:\\
\begin{align*}
E[\hat \lambda]-\lambda&=E[\frac{1}{n}\sum_{i=1}^n X_i]-\lambda\\
							&=\frac{1}{n}\sum_{i=1}^n E[X_i]-\lambda\\
							&=\frac{1}{n}\sum_{i=1}^n \lambda-\lambda\\
							&=0
\end{align*}



 



\subsection{Problem 2}
Gamma distribution has pdf:
\begin{displaymath}
p(\lambda|\alpha,\beta)=\frac{\beta^{\alpha}}{\tau(\alpha)}\lambda^{\alpha-1}e^{-\beta \lambda},\lambda>0
\end{displaymath}
For Gamma distribution, the P could be:
\begin{displaymath}
P(\lambda|\sum_{i=1}^{n} X_i+\alpha,n+\beta)=\frac{(n+\beta)^{(\sum_{i=1}^{n}X_i+\alpha)}}{\tau(\sum_{i=1}^{n}X_i+\alpha)}\lambda^{\sum_{i=1}^{n}X_i+\alpha-1}e^{-(n+\beta) \lambda}
\end{displaymath}
Using logarithm:
\begin{displaymath}
\log P(\lambda|\alpha,\beta)=(\sum_{i=1}^{n} X_i+\alpha)\log P(n+\beta)-\log P(\sum_{i=1}^{n} X_i+\alpha)+(\sum_{i=1}^{n} X_i+\alpha-1)\log \lambda-(n+\beta)\lambda
\end{displaymath}
Finally, get the derivative=0 of the log function:
\begin{align*}
(\sum_{i=1}^{n} X_i+\alpha-1)-(n+\beta)=0\\
\lambda_{MAP}=\frac{\sum_{i=1}^{n} X_i+\alpha-1}{n+\beta}
\end{align*}


%----------------------------------------------------------------------------------------
%	SECTION 3 Question 2
%----------------------------------------------------------------------------------------

\section{Decision Trees}
\subsection{H(Y)}
The total count is $3+4+4+1+0+1+3+5=21$. \\
$P(Y=+)=\frac{12}{21}=0.57$,$P(Y=-)=\frac{9}{21}=0.43$.\\
Entropy for Y is:
\begin{align*}
H(Y)&=-\sum P(Y)\log_2 P(Y)\\
	 &=-P(Y=+)\log_2 P(Y=+)-P(Y=-)\log_2 P(Y=-)\\
	 &=-0.57*-0.81-0.43*-1.2\\
	 &=0.98
\end{align*}

\subsection{Information gain}
In the training data: $P(X_1=T)=\frac{8}{21}=0.38$,$P(X_1=F)=\frac{13}{21}=0.62$.\\
Computing the information gain $IG(X_1)$:
\begin{align*}
IG(X_1)&=H(Y)-H(Y|X_1)\\
		&=H(Y)-(P(X_1=T)H(Y|X_1=T)+P(X_1=F)H(Y|X_1=F))\\
		&=0.98-0.38({-\frac{7}{8}\log_2 \frac{7}{8}}-{\frac{1}{8}\log_2 \frac{1}{8}})-0.62({-\frac{5}{13}\log_2 \frac{5}{13}}-{\frac{8}{13}\log_2 \frac{8}{13}})\\
		&=0.98-0.38(0.17+0.38)-0.62(0.43+0.53)\\
		&=0.98-0.21-0.60\\
		&=0.17
\end{align*}\\
For $X_2$: $P(X_2=T)=\frac{10}{21}=0.48$; $P(X_2=F)=\frac{11}{21}=0.52$.\\
\begin{align*}
IG(X_2)&=H(Y)-H(Y|X_2)\\
		&=H(Y)-(P(X_2=T)H(Y|X_2=T)+P(X_2=F)H(Y|X_2=F))\\
		&=0.98-0.48({-\frac{7}{10}\log_2 \frac{7}{10}}-{-\frac{3}{10}\log_2 \frac{3}{10}})-0.52({\frac{5}{11}\log_2 \frac{5}{11}}-{\frac{6}{11}\log_2 \frac{6}{11}})\\
		&=0.98-0.48(0.36+0.52)-0.52(0.52+0.47)\\
		&=0.98-0.42-0.51\\
		&=0.05
\end{align*}

\subsection{Decision Tree}
\begin{figure}[H]
\centering
\includegraphics[scale=01]{id3.jpg}
\caption{ID3 Tree}
\label{fig3.2.1}
\end{figure}


%----------------------------------------------------------------------------------------
%	SECTION 4 Question 3
%----------------------------------------------------------------------------------------

\section{Perceptron}
\subsection{OR Function}
The truth table for OR function is:\\
\\
\begin{tabular}{|c|c|c|}
\hline
a&b&OR\\
\hline
0&0&0\\
0&1&1\\
1&0&1\\
1&1&1\\
\hline
\end{tabular}\\
\\
In the Perceptron, $Prediction (\dot y) = 1$ if $$Wx+b >= 0$$ and 0 if $$Wx+b<0.$$
For OR function, y=0 only if a=0 and b=0.\\
Assume the function is $y=w_1 x_1+w_2 x_2+b$.\\
Initialize $w_1=1$,$w_2=1$ and $b=-1$.\\
1.$y=x_1 *1+x_2 *1-1$.Passing the first line (a=0,b=0,out=0):$$0+0-1=-1<0,\dot y =0;Correct.$$
2.$y=x_1 *1+x_2 *1-1$.Passing the second line (a=0,b=1,out=1):$$0+1-1=0,\dot y =1;Correct.$$ 
3.$y=x_1 *1+x_2 *1-1$.Passing the third line (a=1,b=0,out=1):$$1+0-1=0,\dot y =1;Correct.$$ 
4.$y=x_1 *1+x_2 *1-1$.Passing the fourth line (a=1,b=1,out=1):$$1+1-1=1,\dot y =1;Correct.$$ 
This shows the perceptron fit OR function well with set of inequalities.


\subsection{XOR Function}
The truth table for XOR function is:\\
\\
\begin{tabular}{|c|c|c|}
\hline
a&b&XOR\\
\hline
0&0&0\\
0&1&1\\
1&0&1\\
1&1&0\\
\hline
\end{tabular}\\
\\
Prove by contradiction:\\
Assume the perceptron could fit XOR function. \\Perceptron is fitting function:$y=Wx+b$, thus it is a linear classifier.\\
So the two category (0,1) should be divided by a line.\\
By observing the truth table, there are at least two lines to seperate 0 and 1 in a plane.\\
That shows the perceptron can't fit XOR function.
%----------------------------------------------------------------------------------------
%	SECTION 5 Question 4
%----------------------------------------------------------------------------------------

\section{Support Vector Machine}
\subsection{Dataset Details}
This project uses Breast Cancer Wisconsin (Diagnostic) Data Set. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\\
The attributes contains ID, Diagnosis(malignant or benign) and ten real-valued features for each cell nucleus.\\
Here we use wdbc.data as our training set.

\subsection{Algorithm Description}
In machine learning, support-vector machines (SVM) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.\\
In this project, some data preprocessing should be done:
\begin{itemize}
\item Transform data file to csv;
\item Insert header to the csv based on the dataset description;
\item Split dataset into train(0.67) and test(0.33) data;
\end{itemize}
The data could be visualized:
\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{breastvisual.png}
\caption{Visualization for Breast Cancer}
\label{fig5.2.1}
\end{figure}

\subsection{Algorithm Results}
Running the algorithm on Python2.7.\\
Using linear kernel, the final score for SVM is 0.98425196850393704.\\
The confusion matrix is:\\
\begin{tabular}{lr}
119&2\\
2&65
\end{tabular}\\
Normalizing the confusion matrix:\\
\begin{tabular}{lr}
0.98&0.02\\
0.03&0.97
\end{tabular}\\
\subsection{Runtime}
The SVM's time complexity for linear kernel is O(nd),n is number of training samples, d is number of feature dimensions.



%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------

%\section{Answers to Definitions}



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\bibliographystyle{apalike}

%\bibliography{}

%----------------------------------------------------------------------------------------


\end{document}