\documentclass{article}

\usepackage{graphicx} % Required for the inclusion of images
%\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage{float}
\usepackage{url}
\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Project 3 Report \\ MNIST} % Title

\author{Xiangyang \textsc{Han}} % Author name

\date{\today} % Date for the report

\begin{document}

\maketitle % Insert the title, author and date


% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Project Overview}


\begin{tabular}{ll}
Name:&Xiangyang Han\\
Student ID:&G29562597\\
Major Resource:&ide:Spyder; Reference:wiki,stackoverflow\\
Programming Language:&python 2.7\\
Github url:&\url{https://github.com/wed91007/project-gwu}
\end{tabular}


% If you have more than one objective, uncomment the below:
%\begin{description}
%\item[First Objective] \hfill \\
%Objective 1 text
%\item[Second Objective] \hfill \\
%Objective 2 text
%\end{description}


 
%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Data Details}

\subsection{MNIST Dataset}
\subsubsection{Overview}

The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The dataset is divided into train set and test set. The first column of each dataset is the label indicating the number is from 0 to 9.\\
In the project, the train.csv is used. It is separated into train set(75\%) and test set(25\%). Besides, 10\% of the train set are used as validation set.

\subsubsection{Visualization}
The data in MNIST are numbers representing gray degree for each pixel in an image labeled 0 to 9.\\
The original data is a 1-dimension array. By reshape the data into a 28*28 gray image, the handwritten digits are shown.\\

\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{mnistvisual.png}
\caption{Visualization of MNIST}
\label{fig2.1}
\end{figure}


%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Algorithm Description}
\subsection{Random Forest}
Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.



\subsection{KNN}

In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\\

\begin{itemize}
\item In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\\
\item In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\\
\end{itemize}


%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Algorithm Results}
\subsection{Random Forest Result}
Taking 1000 data points to train, we check the performance for different numbers of estimators in random forest classifier.\\

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{estimators.png}
\caption{Estimator}
\label{fig4.1}
\end{figure}

From the figure, the performance achieved best at estimator around 100.\\
Using clf.feature\_importance:\\

1. feature 409 (0.008988)\\
2. feature 406 (0.008751)\\
3. feature 378 (0.008357)\\
4. feature 433 (0.007776)\\
5. feature 405 (0.007709)\\
6. feature 350 (0.007065)\\
7. feature 347 (0.006894)\\
8. feature 155 (0.006886)\\
9. feature 437 (0.006714)\\
10. feature 377 (0.006212)\\

From the table of feature importance, there is no significantly important features in the original data. Then we implement PCA.\\
Increase the number of components in PCA from [1,2,3,4,5,10,20,50,100,200,500].\\

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{compnumber.png}
\caption{Components numbers for PCA}
\label{fig4.2}
\end{figure}

Component number around 100 captured 90\% of the variance in the data.\\



\subsection{KNN Result}
Implement KNN on PCA output.\\

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{knnpca.png}
\caption{KNN on PCA Data}
\label{fig4.2}
\end{figure}
The result is similar to the random forest with accuracy around 90\%. And there is a drop on accuracy when number of components become bigger probably due to overfitting.\\

%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Runtime}
For sample=n, feature=m, time complexity is $O(mn \log n)$.\\
The program takes 29s to compute.




%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------

%\section{Answers to Definitions}



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\bibliographystyle{apalike}

%\bibliography{}

%----------------------------------------------------------------------------------------


\end{document}